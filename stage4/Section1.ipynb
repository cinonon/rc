{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incomplete-agreement",
   "metadata": {},
   "source": [
    "Section1：再帰型ニューラルネットワークの概念"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "anonymous-charity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iters:0\n",
      "Loss:1.2580713295358779\n",
      "A [0 1 1 1 1 1 1 0]  B [0 0 1 1 0 0 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 1 0 0 0 1]\n",
      "126 + 51 = 0\n",
      "------------\n",
      "iters:100\n",
      "Loss:1.0505807801713463\n",
      "A [0 0 0 0 1 1 0 0]  B [0 0 0 1 1 0 0 0]\n",
      "Pred:[1 0 0 0 0 0 1 1]\n",
      "True:[0 0 1 0 0 1 0 0]\n",
      "12 + 24 = 131\n",
      "------------\n",
      "iters:200\n",
      "Loss:0.9578975707702252\n",
      "A [0 0 0 1 1 0 1 0]  B [0 0 1 0 1 0 1 1]\n",
      "Pred:[1 0 1 0 0 1 0 1]\n",
      "True:[0 1 0 0 0 1 0 1]\n",
      "26 + 43 = 165\n",
      "------------\n",
      "iters:300\n",
      "Loss:1.0658614307139593\n",
      "A [0 0 0 0 1 1 1 1]  B [0 1 0 1 1 1 0 1]\n",
      "Pred:[1 1 1 1 1 1 1 0]\n",
      "True:[0 1 1 0 1 1 0 0]\n",
      "15 + 93 = 254\n",
      "------------\n",
      "iters:400\n",
      "Loss:1.0469750452805124\n",
      "A [0 1 0 0 1 0 0 0]  B [0 0 0 0 1 0 0 0]\n",
      "Pred:[1 1 1 1 1 0 1 0]\n",
      "True:[0 1 0 1 0 0 0 0]\n",
      "72 + 8 = 250\n",
      "------------\n",
      "iters:500\n",
      "Loss:0.9265058781920513\n",
      "A [0 1 1 0 0 1 1 1]  B [0 0 0 1 0 1 1 1]\n",
      "Pred:[1 1 1 1 1 1 1 0]\n",
      "True:[0 1 1 1 1 1 1 0]\n",
      "103 + 23 = 254\n",
      "------------\n",
      "iters:600\n",
      "Loss:1.381032177548102\n",
      "A [0 1 1 1 1 1 0 1]  B [0 1 1 1 0 1 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 1 1 1 0 0 1 1]\n",
      "125 + 118 = 0\n",
      "------------\n",
      "iters:700\n",
      "Loss:1.0728177282091604\n",
      "A [0 0 1 0 0 1 0 0]  B [0 1 1 1 1 1 0 0]\n",
      "Pred:[0 1 0 0 1 0 0 0]\n",
      "True:[1 0 1 0 0 0 0 0]\n",
      "36 + 124 = 72\n",
      "------------\n",
      "iters:800\n",
      "Loss:1.1686464023930005\n",
      "A [0 1 0 1 0 0 0 0]  B [0 0 1 1 1 1 1 0]\n",
      "Pred:[0 1 1 0 0 0 0 0]\n",
      "True:[1 0 0 0 1 1 1 0]\n",
      "80 + 62 = 96\n",
      "------------\n",
      "iters:900\n",
      "Loss:0.8946563337569389\n",
      "A [0 0 0 0 0 1 1 1]  B [0 1 0 0 0 1 0 1]\n",
      "Pred:[0 0 0 0 0 0 1 0]\n",
      "True:[0 1 0 0 1 1 0 0]\n",
      "7 + 69 = 2\n",
      "------------\n",
      "iters:1000\n",
      "Loss:1.3705776443621724\n",
      "A [0 0 0 1 1 1 1 0]  B [0 1 1 0 0 0 1 1]\n",
      "Pred:[1 1 1 1 1 1 1 1]\n",
      "True:[1 0 0 0 0 0 0 1]\n",
      "30 + 99 = 255\n",
      "------------\n",
      "iters:1100\n",
      "Loss:0.7612582299393296\n",
      "A [0 1 0 0 0 0 0 1]  B [0 1 1 1 0 1 1 0]\n",
      "Pred:[1 1 1 1 0 0 1 1]\n",
      "True:[1 0 1 1 0 1 1 1]\n",
      "65 + 118 = 243\n",
      "------------\n",
      "iters:1200\n",
      "Loss:0.8855116040943816\n",
      "A [0 1 0 0 0 0 0 0]  B [0 0 0 0 1 1 1 1]\n",
      "Pred:[1 1 0 1 1 1 1 1]\n",
      "True:[0 1 0 0 1 1 1 1]\n",
      "64 + 15 = 223\n",
      "------------\n",
      "iters:1300\n",
      "Loss:0.8393134285234274\n",
      "A [0 1 0 0 0 1 0 0]  B [0 0 0 1 0 1 0 1]\n",
      "Pred:[1 0 0 1 1 0 0 0]\n",
      "True:[0 1 0 1 1 0 0 1]\n",
      "68 + 21 = 152\n",
      "------------\n",
      "iters:1400\n",
      "Loss:1.0953755020959006\n",
      "A [0 1 1 1 0 1 1 1]  B [0 0 1 0 1 0 1 0]\n",
      "Pred:[0 1 0 1 1 1 0 1]\n",
      "True:[1 0 1 0 0 0 0 1]\n",
      "119 + 42 = 93\n",
      "------------\n",
      "iters:1500\n",
      "Loss:0.7512923451183537\n",
      "A [0 1 1 0 0 0 0 0]  B [0 1 1 0 1 1 1 1]\n",
      "Pred:[1 1 0 1 1 1 1 0]\n",
      "True:[1 1 0 0 1 1 1 1]\n",
      "96 + 111 = 222\n",
      "------------\n",
      "iters:1600\n",
      "Loss:0.782171014315193\n",
      "A [0 0 0 0 1 1 0 0]  B [0 1 0 0 1 0 1 1]\n",
      "Pred:[1 1 0 1 1 1 1 1]\n",
      "True:[0 1 0 1 0 1 1 1]\n",
      "12 + 75 = 223\n",
      "------------\n",
      "iters:1700\n",
      "Loss:0.6034738821063609\n",
      "A [0 0 0 0 1 0 1 0]  B [0 0 1 1 0 0 0 0]\n",
      "Pred:[0 0 1 1 1 0 1 0]\n",
      "True:[0 0 1 1 1 0 1 0]\n",
      "10 + 48 = 58\n",
      "------------\n",
      "iters:1800\n",
      "Loss:0.9085259964998544\n",
      "A [0 1 0 1 1 1 0 1]  B [0 0 1 0 0 1 0 1]\n",
      "Pred:[0 1 0 0 0 0 0 0]\n",
      "True:[1 0 0 0 0 0 1 0]\n",
      "93 + 37 = 64\n",
      "------------\n",
      "iters:1900\n",
      "Loss:0.5110724608291897\n",
      "A [0 0 1 1 0 1 1 1]  B [0 0 1 0 0 0 0 0]\n",
      "Pred:[0 1 0 1 0 1 1 1]\n",
      "True:[0 1 0 1 0 1 1 1]\n",
      "55 + 32 = 87\n",
      "------------\n",
      "iters:2000\n",
      "Loss:0.3893174021534668\n",
      "A [0 1 0 1 0 0 0 1]  B [0 0 0 0 0 0 0 0]\n",
      "Pred:[0 1 0 1 0 0 0 1]\n",
      "True:[0 1 0 1 0 0 0 1]\n",
      "81 + 0 = 81\n",
      "------------\n",
      "iters:2100\n",
      "Loss:0.4722438419981241\n",
      "A [0 0 1 0 1 0 1 0]  B [0 0 1 1 0 1 0 0]\n",
      "Pred:[0 1 0 1 1 1 1 0]\n",
      "True:[0 1 0 1 1 1 1 0]\n",
      "42 + 52 = 94\n",
      "------------\n",
      "iters:2200\n",
      "Loss:0.36176423015117104\n",
      "A [0 1 0 0 0 0 1 1]  B [0 0 0 0 1 0 1 1]\n",
      "Pred:[0 1 0 0 1 1 1 0]\n",
      "True:[0 1 0 0 1 1 1 0]\n",
      "67 + 11 = 78\n",
      "------------\n",
      "iters:2300\n",
      "Loss:0.40181528022152824\n",
      "A [0 0 0 0 0 0 1 0]  B [0 1 1 0 0 0 1 1]\n",
      "Pred:[0 1 1 0 0 1 0 1]\n",
      "True:[0 1 1 0 0 1 0 1]\n",
      "2 + 99 = 101\n",
      "------------\n",
      "iters:2400\n",
      "Loss:0.6207939229574164\n",
      "A [0 0 1 1 1 1 1 0]  B [0 0 0 0 1 1 0 1]\n",
      "Pred:[0 1 0 0 1 0 1 1]\n",
      "True:[0 1 0 0 1 0 1 1]\n",
      "62 + 13 = 75\n",
      "------------\n",
      "iters:2500\n",
      "Loss:0.8299621614531011\n",
      "A [0 1 0 0 1 0 1 0]  B [0 0 1 0 0 1 1 0]\n",
      "Pred:[0 1 1 1 1 1 0 0]\n",
      "True:[0 1 1 1 0 0 0 0]\n",
      "74 + 38 = 124\n",
      "------------\n",
      "iters:2600\n",
      "Loss:0.23653712054882847\n",
      "A [0 1 0 0 1 0 0 0]  B [0 1 0 0 1 0 0 1]\n",
      "Pred:[1 0 0 1 0 0 0 1]\n",
      "True:[1 0 0 1 0 0 0 1]\n",
      "72 + 73 = 145\n",
      "------------\n",
      "iters:2700\n",
      "Loss:0.2841192469957402\n",
      "A [0 0 0 1 1 1 1 0]  B [0 1 0 0 1 0 0 0]\n",
      "Pred:[0 1 1 0 0 1 1 0]\n",
      "True:[0 1 1 0 0 1 1 0]\n",
      "30 + 72 = 102\n",
      "------------\n",
      "iters:2800\n",
      "Loss:0.8858480837568725\n",
      "A [0 1 0 0 1 0 0 1]  B [0 0 1 1 1 1 1 0]\n",
      "Pred:[1 1 1 1 0 1 1 1]\n",
      "True:[1 0 0 0 0 1 1 1]\n",
      "73 + 62 = 247\n",
      "------------\n",
      "iters:2900\n",
      "Loss:0.5905813306915308\n",
      "A [0 1 0 1 0 0 1 1]  B [0 0 1 1 0 1 1 1]\n",
      "Pred:[1 1 0 0 1 1 1 0]\n",
      "True:[1 0 0 0 1 0 1 0]\n",
      "83 + 55 = 206\n",
      "------------\n",
      "iters:3000\n",
      "Loss:0.26033705955009195\n",
      "A [0 1 1 0 1 0 1 0]  B [0 0 0 0 0 0 1 1]\n",
      "Pred:[0 1 1 0 1 1 0 1]\n",
      "True:[0 1 1 0 1 1 0 1]\n",
      "106 + 3 = 109\n",
      "------------\n",
      "iters:3100\n",
      "Loss:0.3842999891358973\n",
      "A [0 1 0 1 1 1 0 1]  B [0 0 1 1 0 1 0 0]\n",
      "Pred:[1 0 0 1 0 0 0 1]\n",
      "True:[1 0 0 1 0 0 0 1]\n",
      "93 + 52 = 145\n",
      "------------\n",
      "iters:3200\n",
      "Loss:0.09718830943773449\n",
      "A [0 1 0 0 0 1 0 1]  B [0 0 1 0 0 0 0 0]\n",
      "Pred:[0 1 1 0 0 1 0 1]\n",
      "True:[0 1 1 0 0 1 0 1]\n",
      "69 + 32 = 101\n",
      "------------\n",
      "iters:3300\n",
      "Loss:0.12201801620816786\n",
      "A [0 0 0 1 0 0 0 1]  B [0 1 0 1 0 0 1 1]\n",
      "Pred:[0 1 1 0 0 1 0 0]\n",
      "True:[0 1 1 0 0 1 0 0]\n",
      "17 + 83 = 100\n",
      "------------\n",
      "iters:3400\n",
      "Loss:0.12540185734119969\n",
      "A [0 0 0 0 0 1 1 1]  B [0 1 0 1 0 0 0 0]\n",
      "Pred:[0 1 0 1 0 1 1 1]\n",
      "True:[0 1 0 1 0 1 1 1]\n",
      "7 + 80 = 87\n",
      "------------\n",
      "iters:3500\n",
      "Loss:0.1238333213111773\n",
      "A [0 1 1 0 1 0 1 1]  B [0 0 1 1 1 0 1 1]\n",
      "Pred:[1 0 1 0 0 1 1 0]\n",
      "True:[1 0 1 0 0 1 1 0]\n",
      "107 + 59 = 166\n",
      "------------\n",
      "iters:3600\n",
      "Loss:0.24912982522711086\n",
      "A [0 1 1 0 1 1 0 1]  B [0 1 1 1 1 1 1 1]\n",
      "Pred:[1 1 1 0 1 1 0 0]\n",
      "True:[1 1 1 0 1 1 0 0]\n",
      "109 + 127 = 236\n",
      "------------\n",
      "iters:3700\n",
      "Loss:0.10395517295348508\n",
      "A [0 1 0 1 0 0 0 1]  B [0 0 1 0 1 0 1 1]\n",
      "Pred:[0 1 1 1 1 1 0 0]\n",
      "True:[0 1 1 1 1 1 0 0]\n",
      "81 + 43 = 124\n",
      "------------\n",
      "iters:3800\n",
      "Loss:0.3319927374400554\n",
      "A [0 0 0 1 0 1 1 1]  B [0 1 1 1 1 1 1 1]\n",
      "Pred:[1 1 0 1 0 1 1 0]\n",
      "True:[1 0 0 1 0 1 1 0]\n",
      "23 + 127 = 214\n",
      "------------\n",
      "iters:3900\n",
      "Loss:0.0790156104012447\n",
      "A [0 0 0 1 1 1 0 0]  B [0 0 0 1 0 1 1 1]\n",
      "Pred:[0 0 1 1 0 0 1 1]\n",
      "True:[0 0 1 1 0 0 1 1]\n",
      "28 + 23 = 51\n",
      "------------\n",
      "iters:4000\n",
      "Loss:0.1037482732361331\n",
      "A [0 0 1 0 1 0 0 0]  B [0 1 0 0 0 1 1 1]\n",
      "Pred:[0 1 1 0 1 1 1 1]\n",
      "True:[0 1 1 0 1 1 1 1]\n",
      "40 + 71 = 111\n",
      "------------\n",
      "iters:4100\n",
      "Loss:0.020989606358251315\n",
      "A [0 0 1 1 1 1 0 1]  B [0 0 0 0 0 0 0 0]\n",
      "Pred:[0 0 1 1 1 1 0 1]\n",
      "True:[0 0 1 1 1 1 0 1]\n",
      "61 + 0 = 61\n",
      "------------\n",
      "iters:4200\n",
      "Loss:0.03250361540994862\n",
      "A [0 0 1 0 1 0 1 0]  B [0 1 0 1 0 1 0 0]\n",
      "Pred:[0 1 1 1 1 1 1 0]\n",
      "True:[0 1 1 1 1 1 1 0]\n",
      "42 + 84 = 126\n",
      "------------\n",
      "iters:4300\n",
      "Loss:0.11606397864912231\n",
      "A [0 1 0 0 1 1 0 0]  B [0 1 1 1 0 1 1 0]\n",
      "Pred:[1 1 0 0 0 0 1 0]\n",
      "True:[1 1 0 0 0 0 1 0]\n",
      "76 + 118 = 194\n",
      "------------\n",
      "iters:4400\n",
      "Loss:0.09055556935029474\n",
      "A [0 0 0 1 1 0 1 1]  B [0 1 1 1 1 0 0 0]\n",
      "Pred:[1 0 0 1 0 0 1 1]\n",
      "True:[1 0 0 1 0 0 1 1]\n",
      "27 + 120 = 147\n",
      "------------\n",
      "iters:4500\n",
      "Loss:0.04058112387124993\n",
      "A [0 0 1 0 0 1 0 1]  B [0 0 0 0 1 1 0 0]\n",
      "Pred:[0 0 1 1 0 0 0 1]\n",
      "True:[0 0 1 1 0 0 0 1]\n",
      "37 + 12 = 49\n",
      "------------\n",
      "iters:4600\n",
      "Loss:0.022508774297915233\n",
      "A [0 1 0 1 1 0 0 0]  B [0 0 0 0 0 0 0 1]\n",
      "Pred:[0 1 0 1 1 0 0 1]\n",
      "True:[0 1 0 1 1 0 0 1]\n",
      "88 + 1 = 89\n",
      "------------\n",
      "iters:4700\n",
      "Loss:0.03343048658653846\n",
      "A [0 0 1 1 1 1 0 1]  B [0 1 0 1 1 1 0 1]\n",
      "Pred:[1 0 0 1 1 0 1 0]\n",
      "True:[1 0 0 1 1 0 1 0]\n",
      "61 + 93 = 154\n",
      "------------\n",
      "iters:4800\n",
      "Loss:0.026714145599479194\n",
      "A [0 1 1 1 0 0 0 1]  B [0 0 0 1 1 0 1 0]\n",
      "Pred:[1 0 0 0 1 0 1 1]\n",
      "True:[1 0 0 0 1 0 1 1]\n",
      "113 + 26 = 139\n",
      "------------\n",
      "iters:4900\n",
      "Loss:0.024826718168774842\n",
      "A [0 1 1 1 1 0 1 0]  B [0 0 1 1 1 0 1 0]\n",
      "Pred:[1 0 1 1 0 1 0 0]\n",
      "True:[1 0 1 1 0 1 0 0]\n",
      "122 + 58 = 180\n",
      "------------\n",
      "iters:5000\n",
      "Loss:0.0277929681846664\n",
      "A [0 0 0 1 1 1 0 0]  B [0 1 0 0 1 0 1 1]\n",
      "Pred:[0 1 1 0 0 1 1 1]\n",
      "True:[0 1 1 0 0 1 1 1]\n",
      "28 + 75 = 103\n",
      "------------\n",
      "iters:5100\n",
      "Loss:0.009531809490808998\n",
      "A [0 0 0 1 0 1 1 1]  B [0 1 0 1 1 1 0 0]\n",
      "Pred:[0 1 1 1 0 0 1 1]\n",
      "True:[0 1 1 1 0 0 1 1]\n",
      "23 + 92 = 115\n",
      "------------\n",
      "iters:5200\n",
      "Loss:0.017839654608008744\n",
      "A [0 0 0 1 1 1 1 0]  B [0 1 0 0 0 1 1 1]\n",
      "Pred:[0 1 1 0 0 1 0 1]\n",
      "True:[0 1 1 0 0 1 0 1]\n",
      "30 + 71 = 101\n",
      "------------\n",
      "iters:5300\n",
      "Loss:0.025308386732647577\n",
      "A [0 0 1 1 1 0 1 0]  B [0 0 1 1 1 0 0 1]\n",
      "Pred:[0 1 1 1 0 0 1 1]\n",
      "True:[0 1 1 1 0 0 1 1]\n",
      "58 + 57 = 115\n",
      "------------\n",
      "iters:5400\n",
      "Loss:0.015429563024338008\n",
      "A [0 0 1 1 1 0 0 0]  B [0 1 0 1 0 0 0 0]\n",
      "Pred:[1 0 0 0 1 0 0 0]\n",
      "True:[1 0 0 0 1 0 0 0]\n",
      "56 + 80 = 136\n",
      "------------\n",
      "iters:5500\n",
      "Loss:0.013701090712254727\n",
      "A [0 1 0 1 1 1 1 1]  B [0 0 1 0 1 0 1 1]\n",
      "Pred:[1 0 0 0 1 0 1 0]\n",
      "True:[1 0 0 0 1 0 1 0]\n",
      "95 + 43 = 138\n",
      "------------\n",
      "iters:5600\n",
      "Loss:0.009665045535712463\n",
      "A [0 1 0 0 0 0 1 0]  B [0 0 0 0 1 0 1 0]\n",
      "Pred:[0 1 0 0 1 1 0 0]\n",
      "True:[0 1 0 0 1 1 0 0]\n",
      "66 + 10 = 76\n",
      "------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iters:5700\n",
      "Loss:0.009537292088069966\n",
      "A [0 1 0 0 0 0 0 1]  B [0 0 1 1 1 0 1 0]\n",
      "Pred:[0 1 1 1 1 0 1 1]\n",
      "True:[0 1 1 1 1 0 1 1]\n",
      "65 + 58 = 123\n",
      "------------\n",
      "iters:5800\n",
      "Loss:0.004675961977043778\n",
      "A [0 0 1 0 0 0 1 1]  B [0 1 0 0 1 0 1 0]\n",
      "Pred:[0 1 1 0 1 1 0 1]\n",
      "True:[0 1 1 0 1 1 0 1]\n",
      "35 + 74 = 109\n",
      "------------\n",
      "iters:5900\n",
      "Loss:0.006605941038279534\n",
      "A [0 1 1 0 0 1 1 1]  B [0 0 1 0 1 0 0 1]\n",
      "Pred:[1 0 0 1 0 0 0 0]\n",
      "True:[1 0 0 1 0 0 0 0]\n",
      "103 + 41 = 144\n",
      "------------\n",
      "iters:6000\n",
      "Loss:0.008410058654069354\n",
      "A [0 1 0 1 1 0 0 0]  B [0 0 1 0 0 1 0 0]\n",
      "Pred:[0 1 1 1 1 1 0 0]\n",
      "True:[0 1 1 1 1 1 0 0]\n",
      "88 + 36 = 124\n",
      "------------\n",
      "iters:6100\n",
      "Loss:0.011386078009587262\n",
      "A [0 0 0 0 0 0 1 0]  B [0 0 0 0 0 1 1 0]\n",
      "Pred:[0 0 0 0 1 0 0 0]\n",
      "True:[0 0 0 0 1 0 0 0]\n",
      "2 + 6 = 8\n",
      "------------\n",
      "iters:6200\n",
      "Loss:0.015863708082008064\n",
      "A [0 0 0 1 1 1 0 1]  B [0 1 0 1 1 0 1 1]\n",
      "Pred:[0 1 1 1 1 0 0 0]\n",
      "True:[0 1 1 1 1 0 0 0]\n",
      "29 + 91 = 120\n",
      "------------\n",
      "iters:6300\n",
      "Loss:0.008671726287569664\n",
      "A [0 0 0 0 1 1 1 1]  B [0 1 1 1 0 1 0 0]\n",
      "Pred:[1 0 0 0 0 0 1 1]\n",
      "True:[1 0 0 0 0 0 1 1]\n",
      "15 + 116 = 131\n",
      "------------\n",
      "iters:6400\n",
      "Loss:0.02025764229187327\n",
      "A [0 0 1 0 1 0 1 0]  B [0 0 1 1 1 1 1 0]\n",
      "Pred:[0 1 1 0 1 0 0 0]\n",
      "True:[0 1 1 0 1 0 0 0]\n",
      "42 + 62 = 104\n",
      "------------\n",
      "iters:6500\n",
      "Loss:0.010500872078361603\n",
      "A [0 1 1 0 0 1 1 0]  B [0 1 1 1 0 1 1 0]\n",
      "Pred:[1 1 0 1 1 1 0 0]\n",
      "True:[1 1 0 1 1 1 0 0]\n",
      "102 + 118 = 220\n",
      "------------\n",
      "iters:6600\n",
      "Loss:0.0035191729506396374\n",
      "A [0 0 0 0 0 1 1 1]  B [0 0 0 1 1 0 0 1]\n",
      "Pred:[0 0 1 0 0 0 0 0]\n",
      "True:[0 0 1 0 0 0 0 0]\n",
      "7 + 25 = 32\n",
      "------------\n",
      "iters:6700\n",
      "Loss:0.009784337328734237\n",
      "A [0 1 0 0 1 0 0 0]  B [0 1 0 0 0 1 1 1]\n",
      "Pred:[1 0 0 0 1 1 1 1]\n",
      "True:[1 0 0 0 1 1 1 1]\n",
      "72 + 71 = 143\n",
      "------------\n",
      "iters:6800\n",
      "Loss:0.009685032619705885\n",
      "A [0 1 0 0 1 0 0 0]  B [0 0 1 1 1 0 1 1]\n",
      "Pred:[1 0 0 0 0 0 1 1]\n",
      "True:[1 0 0 0 0 0 1 1]\n",
      "72 + 59 = 131\n",
      "------------\n",
      "iters:6900\n",
      "Loss:0.006361093646506833\n",
      "A [0 0 1 0 1 0 1 1]  B [0 0 1 1 0 0 1 1]\n",
      "Pred:[0 1 0 1 1 1 1 0]\n",
      "True:[0 1 0 1 1 1 1 0]\n",
      "43 + 51 = 94\n",
      "------------\n",
      "iters:7000\n",
      "Loss:0.005230698782613278\n",
      "A [0 0 0 0 0 0 1 0]  B [0 1 0 1 1 0 1 0]\n",
      "Pred:[0 1 0 1 1 1 0 0]\n",
      "True:[0 1 0 1 1 1 0 0]\n",
      "2 + 90 = 92\n",
      "------------\n",
      "iters:7100\n",
      "Loss:0.0017526020582864436\n",
      "A [0 1 1 0 1 1 1 1]  B [0 1 0 0 1 1 0 0]\n",
      "Pred:[1 0 1 1 1 0 1 1]\n",
      "True:[1 0 1 1 1 0 1 1]\n",
      "111 + 76 = 187\n",
      "------------\n",
      "iters:7200\n",
      "Loss:0.0044720314463069085\n",
      "A [0 1 1 1 1 0 0 0]  B [0 0 0 1 1 0 0 0]\n",
      "Pred:[1 0 0 1 0 0 0 0]\n",
      "True:[1 0 0 1 0 0 0 0]\n",
      "120 + 24 = 144\n",
      "------------\n",
      "iters:7300\n",
      "Loss:0.005249644930185405\n",
      "A [0 0 0 0 1 1 0 0]  B [0 1 0 0 0 0 1 1]\n",
      "Pred:[0 1 0 0 1 1 1 1]\n",
      "True:[0 1 0 0 1 1 1 1]\n",
      "12 + 67 = 79\n",
      "------------\n",
      "iters:7400\n",
      "Loss:0.0016055931057602285\n",
      "A [0 0 0 1 0 0 1 1]  B [0 1 0 0 0 1 0 1]\n",
      "Pred:[0 1 0 1 1 0 0 0]\n",
      "True:[0 1 0 1 1 0 0 0]\n",
      "19 + 69 = 88\n",
      "------------\n",
      "iters:7500\n",
      "Loss:0.0033659150403524043\n",
      "A [0 0 1 1 1 1 1 1]  B [0 0 1 0 1 1 1 1]\n",
      "Pred:[0 1 1 0 1 1 1 0]\n",
      "True:[0 1 1 0 1 1 1 0]\n",
      "63 + 47 = 110\n",
      "------------\n",
      "iters:7600\n",
      "Loss:0.0020181130965674947\n",
      "A [0 0 1 0 0 1 0 1]  B [0 0 1 1 1 1 0 1]\n",
      "Pred:[0 1 1 0 0 0 1 0]\n",
      "True:[0 1 1 0 0 0 1 0]\n",
      "37 + 61 = 98\n",
      "------------\n",
      "iters:7700\n",
      "Loss:0.0023616964638508943\n",
      "A [0 1 1 0 0 0 0 0]  B [0 1 0 0 1 0 0 1]\n",
      "Pred:[1 0 1 0 1 0 0 1]\n",
      "True:[1 0 1 0 1 0 0 1]\n",
      "96 + 73 = 169\n",
      "------------\n",
      "iters:7800\n",
      "Loss:0.0015571726943505854\n",
      "A [0 0 0 0 0 0 1 1]  B [0 1 1 1 1 0 1 0]\n",
      "Pred:[0 1 1 1 1 1 0 1]\n",
      "True:[0 1 1 1 1 1 0 1]\n",
      "3 + 122 = 125\n",
      "------------\n",
      "iters:7900\n",
      "Loss:0.0013847668603487707\n",
      "A [0 1 1 1 1 1 1 1]  B [0 0 0 0 1 1 0 1]\n",
      "Pred:[1 0 0 0 1 1 0 0]\n",
      "True:[1 0 0 0 1 1 0 0]\n",
      "127 + 13 = 140\n",
      "------------\n",
      "iters:8000\n",
      "Loss:0.0026404404915161598\n",
      "A [0 1 1 0 0 0 0 0]  B [0 1 0 1 0 1 1 0]\n",
      "Pred:[1 0 1 1 0 1 1 0]\n",
      "True:[1 0 1 1 0 1 1 0]\n",
      "96 + 86 = 182\n",
      "------------\n",
      "iters:8100\n",
      "Loss:0.0031697526153678566\n",
      "A [0 0 0 1 0 0 0 0]  B [0 0 1 1 1 1 0 0]\n",
      "Pred:[0 1 0 0 1 1 0 0]\n",
      "True:[0 1 0 0 1 1 0 0]\n",
      "16 + 60 = 76\n",
      "------------\n",
      "iters:8200\n",
      "Loss:0.0021454410198902827\n",
      "A [0 0 0 0 0 1 1 0]  B [0 1 1 0 1 1 1 1]\n",
      "Pred:[0 1 1 1 0 1 0 1]\n",
      "True:[0 1 1 1 0 1 0 1]\n",
      "6 + 111 = 117\n",
      "------------\n",
      "iters:8300\n",
      "Loss:0.0026921689309173384\n",
      "A [0 0 0 0 1 0 0 0]  B [0 1 1 1 0 1 0 0]\n",
      "Pred:[0 1 1 1 1 1 0 0]\n",
      "True:[0 1 1 1 1 1 0 0]\n",
      "8 + 116 = 124\n",
      "------------\n",
      "iters:8400\n",
      "Loss:0.0018117404814337401\n",
      "A [0 1 0 1 0 1 1 0]  B [0 1 0 0 0 1 1 1]\n",
      "Pred:[1 0 0 1 1 1 0 1]\n",
      "True:[1 0 0 1 1 1 0 1]\n",
      "86 + 71 = 157\n",
      "------------\n",
      "iters:8500\n",
      "Loss:0.0007971601165374339\n",
      "A [0 1 0 1 1 1 0 1]  B [0 0 1 0 1 0 0 1]\n",
      "Pred:[1 0 0 0 0 1 1 0]\n",
      "True:[1 0 0 0 0 1 1 0]\n",
      "93 + 41 = 134\n",
      "------------\n",
      "iters:8600\n",
      "Loss:0.0019111878336365974\n",
      "A [0 0 0 1 0 0 0 0]  B [0 1 1 1 0 0 0 1]\n",
      "Pred:[1 0 0 0 0 0 0 1]\n",
      "True:[1 0 0 0 0 0 0 1]\n",
      "16 + 113 = 129\n",
      "------------\n",
      "iters:8700\n",
      "Loss:0.0015014863067948558\n",
      "A [0 1 1 1 1 0 0 1]  B [0 1 0 1 1 1 1 0]\n",
      "Pred:[1 1 0 1 0 1 1 1]\n",
      "True:[1 1 0 1 0 1 1 1]\n",
      "121 + 94 = 215\n",
      "------------\n",
      "iters:8800\n",
      "Loss:0.002872962623731585\n",
      "A [0 1 1 1 0 1 0 0]  B [0 1 0 1 1 1 0 0]\n",
      "Pred:[1 1 0 1 0 0 0 0]\n",
      "True:[1 1 0 1 0 0 0 0]\n",
      "116 + 92 = 208\n",
      "------------\n",
      "iters:8900\n",
      "Loss:0.0008491553251088308\n",
      "A [0 0 1 1 1 0 1 1]  B [0 0 1 0 1 0 1 0]\n",
      "Pred:[0 1 1 0 0 1 0 1]\n",
      "True:[0 1 1 0 0 1 0 1]\n",
      "59 + 42 = 101\n",
      "------------\n",
      "iters:9000\n",
      "Loss:0.0005952496630800466\n",
      "A [0 0 0 0 0 1 0 1]  B [0 1 0 1 0 0 0 0]\n",
      "Pred:[0 1 0 1 0 1 0 1]\n",
      "True:[0 1 0 1 0 1 0 1]\n",
      "5 + 80 = 85\n",
      "------------\n",
      "iters:9100\n",
      "Loss:0.0017532827872382315\n",
      "A [0 1 1 0 1 1 0 0]  B [0 1 0 0 0 1 0 1]\n",
      "Pred:[1 0 1 1 0 0 0 1]\n",
      "True:[1 0 1 1 0 0 0 1]\n",
      "108 + 69 = 177\n",
      "------------\n",
      "iters:9200\n",
      "Loss:0.0022596845112311304\n",
      "A [0 1 1 1 0 1 0 0]  B [0 1 0 0 0 1 1 0]\n",
      "Pred:[1 0 1 1 1 0 1 0]\n",
      "True:[1 0 1 1 1 0 1 0]\n",
      "116 + 70 = 186\n",
      "------------\n",
      "iters:9300\n",
      "Loss:0.0002571434763546779\n",
      "A [0 1 0 1 0 0 0 1]  B [0 0 0 0 1 1 0 1]\n",
      "Pred:[0 1 0 1 1 1 1 0]\n",
      "True:[0 1 0 1 1 1 1 0]\n",
      "81 + 13 = 94\n",
      "------------\n",
      "iters:9400\n",
      "Loss:0.002067008652066168\n",
      "A [0 0 1 1 0 1 0 0]  B [0 0 0 0 1 1 0 1]\n",
      "Pred:[0 1 0 0 0 0 0 1]\n",
      "True:[0 1 0 0 0 0 0 1]\n",
      "52 + 13 = 65\n",
      "------------\n",
      "iters:9500\n",
      "Loss:0.0022448308559890533\n",
      "A [0 1 1 0 0 1 1 0]  B [0 1 0 1 0 1 1 0]\n",
      "Pred:[1 0 1 1 1 1 0 0]\n",
      "True:[1 0 1 1 1 1 0 0]\n",
      "102 + 86 = 188\n",
      "------------\n",
      "iters:9600\n",
      "Loss:0.0011892109373913247\n",
      "A [0 1 1 0 1 1 1 1]  B [0 1 0 0 0 1 1 1]\n",
      "Pred:[1 0 1 1 0 1 1 0]\n",
      "True:[1 0 1 1 0 1 1 0]\n",
      "111 + 71 = 182\n",
      "------------\n",
      "iters:9700\n",
      "Loss:0.0005391798677593522\n",
      "A [0 0 0 0 0 1 1 1]  B [0 1 0 0 1 1 0 1]\n",
      "Pred:[0 1 0 1 0 1 0 0]\n",
      "True:[0 1 0 1 0 1 0 0]\n",
      "7 + 77 = 84\n",
      "------------\n",
      "iters:9800\n",
      "Loss:0.001069488926036491\n",
      "A [0 0 1 0 0 1 1 1]  B [0 1 1 1 0 0 1 1]\n",
      "Pred:[1 0 0 1 1 0 1 0]\n",
      "True:[1 0 0 1 1 0 1 0]\n",
      "39 + 115 = 154\n",
      "------------\n",
      "iters:9900\n",
      "Loss:0.001422587482531114\n",
      "A [0 0 1 0 0 1 1 0]  B [0 0 1 0 0 1 0 1]\n",
      "Pred:[0 1 0 0 1 0 1 1]\n",
      "True:[0 1 0 0 1 0 1 1]\n",
      "38 + 37 = 75\n",
      "------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x287eccdce48>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from common import functions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# def d_tanh(x):\n",
    "\n",
    "\n",
    "\n",
    "# データを用意\n",
    "# 2進数の桁数\n",
    "binary_dim = 8\n",
    "# 最大値 + 1\n",
    "largest_number = pow(2, binary_dim)\n",
    "# largest_numberまで2進数を用意\n",
    "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "\n",
    "input_layer_size = 2\n",
    "hidden_layer_size = 16\n",
    "output_layer_size = 1\n",
    "\n",
    "weight_init_std = 1\n",
    "learning_rate = 0.1\n",
    "\n",
    "iters_num = 10000\n",
    "plot_interval = 100\n",
    "\n",
    "# ウェイト初期化 (バイアスは簡単のため省略)\n",
    "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
    "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
    "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
    "\n",
    "# Xavier\n",
    "\n",
    "\n",
    "# He\n",
    "\n",
    "\n",
    "\n",
    "# 勾配\n",
    "W_in_grad = np.zeros_like(W_in)\n",
    "W_out_grad = np.zeros_like(W_out)\n",
    "W_grad = np.zeros_like(W)\n",
    "\n",
    "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
    "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
    "y = np.zeros((output_layer_size, binary_dim))\n",
    "\n",
    "delta_out = np.zeros((output_layer_size, binary_dim))\n",
    "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
    "\n",
    "all_losses = []\n",
    "\n",
    "for i in range(iters_num):\n",
    "    \n",
    "    # A, B初期化 (a + b = d)\n",
    "    a_int = np.random.randint(largest_number/2)\n",
    "    a_bin = binary[a_int] # binary encoding\n",
    "    b_int = np.random.randint(largest_number/2)\n",
    "    b_bin = binary[b_int] # binary encoding\n",
    "    \n",
    "    # 正解データ\n",
    "    d_int = a_int + b_int\n",
    "    d_bin = binary[d_int]\n",
    "    \n",
    "    # 出力バイナリ\n",
    "    out_bin = np.zeros_like(d_bin)\n",
    "    \n",
    "    # 時系列全体の誤差\n",
    "    all_loss = 0    \n",
    "    \n",
    "    # 時系列ループ\n",
    "    for t in range(binary_dim):\n",
    "        # 入力値\n",
    "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
    "        # 時刻tにおける正解データ\n",
    "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
    "        \n",
    "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
    "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
    "\n",
    "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
    "\n",
    "\n",
    "        #誤差\n",
    "        loss = functions.mean_squared_error(dd, y[:,t])\n",
    "        \n",
    "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * \n",
    "                            functions.d_sigmoid(y[:,t])        \n",
    "        \n",
    "        all_loss += loss\n",
    "\n",
    "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
    "    \n",
    "    \n",
    "    for t in range(binary_dim)[::-1]:\n",
    "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
    "\n",
    "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + \n",
    "                      np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
    "\n",
    "        # 勾配更新\n",
    "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
    "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
    "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
    "    \n",
    "    # 勾配適用\n",
    "    W_in -= learning_rate * W_in_grad\n",
    "    W_out -= learning_rate * W_out_grad\n",
    "    W -= learning_rate * W_grad\n",
    "    \n",
    "    W_in_grad *= 0\n",
    "    W_out_grad *= 0\n",
    "    W_grad *= 0\n",
    "    \n",
    "\n",
    "    if(i % plot_interval == 0):\n",
    "        all_losses.append(all_loss)        \n",
    "        print(\"iters:\" + str(i))\n",
    "        print(\"Loss:\" + str(all_loss))\n",
    "        print(\"A\", a_bin, \" B\", b_bin)\n",
    "        print(\"Pred:\" + str(out_bin))\n",
    "        print(\"True:\" + str(d_bin))\n",
    "        out_int = 0\n",
    "        for index,x in enumerate(reversed(out_bin)):\n",
    "            out_int += x * pow(2, index)\n",
    "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
    "        print(\"------------\")\n",
    "\n",
    "lists = range(0, iters_num, plot_interval)\n",
    "plt.plot(lists, all_losses, label=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-deposit",
   "metadata": {},
   "source": [
    "ReLU(学習できない)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rational-finland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iters:0\n",
      "Loss:18.905375353335053\n",
      "A [0 1 0 0 1 0 0 1]  B [0 0 0 0 1 0 0 0]\n",
      "Pred:[0 0 0 0 0 3 5 0]\n",
      "True:[0 1 0 1 0 0 0 1]\n",
      "73 + 8 = 22\n",
      "------------\n",
      "iters:100\n",
      "Loss:2.5\n",
      "A [0 0 0 0 0 1 0 0]  B [0 1 0 1 0 0 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 0 1 0 1 1 1]\n",
      "4 + 83 = 0\n",
      "------------\n",
      "iters:200\n",
      "Loss:2.0\n",
      "A [0 0 1 0 0 1 1 0]  B [0 0 1 0 1 1 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 0 1 0 0 1 1]\n",
      "38 + 45 = 0\n",
      "------------\n",
      "iters:300\n",
      "Loss:2.0\n",
      "A [0 1 0 0 0 1 0 0]  B [0 1 0 1 1 1 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 0 0 0 1 1]\n",
      "68 + 95 = 0\n",
      "------------\n",
      "iters:400\n",
      "Loss:3.0\n",
      "A [0 1 1 1 1 0 1 1]  B [0 0 0 0 0 0 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 1 1 1 1 1 0]\n",
      "123 + 3 = 0\n",
      "------------\n",
      "iters:500\n",
      "Loss:3.0\n",
      "A [0 0 0 0 0 1 0 1]  B [0 1 1 1 0 0 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 1 1 0 1 1 1]\n",
      "5 + 114 = 0\n",
      "------------\n",
      "iters:600\n",
      "Loss:2.5\n",
      "A [0 0 1 0 1 0 1 1]  B [0 1 1 0 0 1 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 0 1 1 1 1]\n",
      "43 + 100 = 0\n",
      "------------\n",
      "iters:700\n",
      "Loss:2.5\n",
      "A [0 0 1 1 0 1 1 0]  B [0 0 0 0 0 0 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 0 1 1 0 1 1 1]\n",
      "54 + 1 = 0\n",
      "------------\n",
      "iters:800\n",
      "Loss:2.0\n",
      "A [0 1 0 1 1 0 0 0]  B [0 0 0 1 0 1 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 1 0 1 1 0 0]\n",
      "88 + 20 = 0\n",
      "------------\n",
      "iters:900\n",
      "Loss:1.5\n",
      "A [0 1 0 0 0 0 1 0]  B [0 1 0 0 0 0 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 0 0 0 1 1]\n",
      "66 + 65 = 0\n",
      "------------\n",
      "iters:1000\n",
      "Loss:1.5\n",
      "A [0 0 0 1 0 1 1 1]  B [0 1 0 0 0 0 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 0 1 1 0 0 0]\n",
      "23 + 65 = 0\n",
      "------------\n",
      "iters:1100\n",
      "Loss:2.5\n",
      "A [0 1 0 1 1 0 1 0]  B [0 1 0 0 0 0 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 1 1 1 0 1]\n",
      "90 + 67 = 0\n",
      "------------\n",
      "iters:1200\n",
      "Loss:1.5\n",
      "A [0 0 0 1 1 0 0 1]  B [0 1 1 0 1 1 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 0 0 1 1 0]\n",
      "25 + 109 = 0\n",
      "------------\n",
      "iters:1300\n",
      "Loss:2.5\n",
      "A [0 1 1 1 0 0 1 1]  B [0 0 1 1 1 0 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 0 1 1 1 0]\n",
      "115 + 59 = 0\n",
      "------------\n",
      "iters:1400\n",
      "Loss:2.5\n",
      "A [0 1 1 0 0 1 0 0]  B [0 1 0 0 1 0 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 0 1 1 0 1]\n",
      "100 + 73 = 0\n",
      "------------\n",
      "iters:1500\n",
      "Loss:1.5\n",
      "A [0 0 0 0 1 0 1 0]  B [0 1 0 0 0 1 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 0 1 0 0 0 1]\n",
      "10 + 71 = 0\n",
      "------------\n",
      "iters:1600\n",
      "Loss:2.5\n",
      "A [0 0 0 0 1 0 0 1]  B [0 1 0 1 0 1 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 0 1 1 1 0 1]\n",
      "9 + 84 = 0\n",
      "------------\n",
      "iters:1700\n",
      "Loss:2.5\n",
      "A [0 1 0 0 0 1 0 0]  B [0 0 1 0 0 0 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 1 0 0 1 1 1]\n",
      "68 + 35 = 0\n",
      "------------\n",
      "iters:1800\n",
      "Loss:1.5\n",
      "A [0 0 0 1 1 0 1 1]  B [0 1 1 1 1 0 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 1 0 1 0 0]\n",
      "27 + 121 = 0\n",
      "------------\n",
      "iters:1900\n",
      "Loss:1.5\n",
      "A [0 1 1 1 0 0 1 1]  B [0 0 1 1 1 1 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 1 0 0 0 0]\n",
      "115 + 61 = 0\n",
      "------------\n",
      "iters:2000\n",
      "Loss:0.5\n",
      "A [0 1 0 0 1 0 1 0]  B [0 0 1 1 0 1 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 0 0 0 0 0]\n",
      "74 + 54 = 0\n",
      "------------\n",
      "iters:2100\n",
      "Loss:1.0\n",
      "A [0 0 0 0 0 0 0 0]  B [0 0 0 1 0 1 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 0 0 1 0 1 0 0]\n",
      "0 + 20 = 0\n",
      "------------\n",
      "iters:2200\n",
      "Loss:2.5\n",
      "A [0 1 1 1 0 0 1 1]  B [0 1 0 0 1 0 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 1 1 1 0 0]\n",
      "115 + 73 = 0\n",
      "------------\n",
      "iters:2300\n",
      "Loss:1.5\n",
      "A [0 1 1 1 0 1 1 1]  B [0 1 0 0 1 0 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 1 0 0 0 0 1 0]\n",
      "119 + 75 = 0\n",
      "------------\n",
      "iters:2400\n",
      "Loss:2.5\n",
      "A [0 1 1 0 0 1 0 0]  B [0 1 0 0 1 1 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 1 0 0 1 1]\n",
      "100 + 79 = 0\n",
      "------------\n",
      "iters:2500\n",
      "Loss:2.0\n",
      "A [0 0 0 0 0 1 1 1]  B [0 0 1 0 0 1 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 0 1 0 1 0 1 1]\n",
      "7 + 36 = 0\n",
      "------------\n",
      "iters:2600\n",
      "Loss:1.0\n",
      "A [0 1 1 0 1 0 0 1]  B [0 0 1 1 0 1 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 0 0 0 0 0]\n",
      "105 + 55 = 0\n",
      "------------\n",
      "iters:2700\n",
      "Loss:3.0\n",
      "A [0 1 0 1 1 0 1 0]  B [0 1 1 1 1 1 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 1 0 1 0 1 1 1]\n",
      "90 + 125 = 0\n",
      "------------\n",
      "iters:2800\n",
      "Loss:2.0\n",
      "A [0 0 0 0 0 0 1 0]  B [0 0 1 0 0 1 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 0 1 0 0 1 1 1]\n",
      "2 + 37 = 0\n",
      "------------\n",
      "iters:2900\n",
      "Loss:2.5\n",
      "A [0 0 1 1 0 0 0 0]  B [0 1 0 0 1 0 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 1 1 1 0 1 0]\n",
      "48 + 74 = 0\n",
      "------------\n",
      "iters:3000\n",
      "Loss:2.0\n",
      "A [0 0 1 0 0 1 0 0]  B [0 1 1 1 0 1 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 1 1 0 0 1]\n",
      "36 + 117 = 0\n",
      "------------\n",
      "iters:3100\n",
      "Loss:2.5\n",
      "A [0 1 1 1 0 1 0 1]  B [0 1 0 1 1 0 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 1 0 0 1 1 0 1]\n",
      "117 + 88 = 0\n",
      "------------\n",
      "iters:3200\n",
      "Loss:1.5\n",
      "A [0 1 1 1 0 0 0 1]  B [0 1 0 1 0 0 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 1 0 0 0 0 0 1]\n",
      "113 + 80 = 0\n",
      "------------\n",
      "iters:3300\n",
      "Loss:2.5\n",
      "A [0 0 0 0 0 1 1 1]  B [0 1 0 1 0 1 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 0 1 1 0 1 1]\n",
      "7 + 84 = 0\n",
      "------------\n",
      "iters:3400\n",
      "Loss:2.0\n",
      "A [0 1 0 0 0 0 1 0]  B [0 1 0 1 0 1 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 1 0 1 1 0]\n",
      "66 + 84 = 0\n",
      "------------\n",
      "iters:3500\n",
      "Loss:0.5\n",
      "A [0 0 0 0 1 0 1 1]  B [0 0 0 1 0 1 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 0 1 0 0 0 0 0]\n",
      "11 + 21 = 0\n",
      "------------\n",
      "iters:3600\n",
      "Loss:2.0\n",
      "A [0 0 1 0 1 1 0 1]  B [0 0 0 1 1 0 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 0 0 0 1 1 1]\n",
      "45 + 26 = 0\n",
      "------------\n",
      "iters:3700\n",
      "Loss:2.5\n",
      "A [0 0 1 0 1 0 1 1]  B [0 0 0 1 0 0 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 0 1 1 1 1 0 1]\n",
      "43 + 18 = 0\n",
      "------------\n",
      "iters:3800\n",
      "Loss:1.0\n",
      "A [0 0 1 0 1 1 0 1]  B [0 0 0 1 0 1 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 0 0 0 0 1 0]\n",
      "45 + 21 = 0\n",
      "------------\n",
      "iters:3900\n",
      "Loss:2.5\n",
      "A [0 1 1 1 1 0 0 1]  B [0 1 0 1 1 1 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 1 0 1 0 1 1 0]\n",
      "121 + 93 = 0\n",
      "------------\n",
      "iters:4000\n",
      "Loss:1.0\n",
      "A [0 1 0 1 1 0 0 0]  B [0 0 1 0 1 0 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 0 0 0 1 0]\n",
      "88 + 42 = 0\n",
      "------------\n",
      "iters:4100\n",
      "Loss:2.5\n",
      "A [0 1 0 0 0 1 0 1]  B [0 0 1 0 1 0 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 1 0 1 1 1 0]\n",
      "69 + 41 = 0\n",
      "------------\n",
      "iters:4200\n",
      "Loss:3.0\n",
      "A [0 0 1 0 0 1 0 0]  B [0 1 1 1 1 0 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 1 1 1 1 1]\n",
      "36 + 123 = 0\n",
      "------------\n",
      "iters:4300\n",
      "Loss:2.5\n",
      "A [0 1 1 1 1 1 0 1]  B [0 1 1 1 0 1 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 1 1 1 0 1 0 0]\n",
      "125 + 119 = 0\n",
      "------------\n",
      "iters:4400\n",
      "Loss:2.5\n",
      "A [0 0 0 0 1 1 1 1]  B [0 1 1 0 1 0 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 1 1 1 0 0 1]\n",
      "15 + 106 = 0\n",
      "------------\n",
      "iters:4500\n",
      "Loss:2.5\n",
      "A [0 1 1 0 0 1 0 1]  B [0 1 1 0 0 0 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 1 0 0 0 1 1 1]\n",
      "101 + 98 = 0\n",
      "------------\n",
      "iters:4600\n",
      "Loss:2.0\n",
      "A [0 1 1 0 0 1 1 0]  B [0 1 0 0 1 0 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 1 0 0 0 1]\n",
      "102 + 75 = 0\n",
      "------------\n",
      "iters:4700\n",
      "Loss:1.5\n",
      "A [0 0 0 0 0 1 0 1]  B [0 1 0 0 1 1 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 0 1 0 1 0 0]\n",
      "5 + 79 = 0\n",
      "------------\n",
      "iters:4800\n",
      "Loss:2.0\n",
      "A [0 0 1 1 0 0 1 1]  B [0 0 1 0 0 1 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 0 1 1 0 0 1]\n",
      "51 + 38 = 0\n",
      "------------\n",
      "iters:4900\n",
      "Loss:2.5\n",
      "A [0 1 0 1 0 1 0 1]  B [0 1 1 0 0 1 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 1 1 1 0 0]\n",
      "85 + 103 = 0\n",
      "------------\n",
      "iters:5000\n",
      "Loss:3.0\n",
      "A [0 1 1 0 0 1 0 1]  B [0 0 0 0 1 0 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 1 0 1 1 1 1]\n",
      "101 + 10 = 0\n",
      "------------\n",
      "iters:5100\n",
      "Loss:2.5\n",
      "A [0 1 0 0 1 1 0 0]  B [0 1 0 0 1 1 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 1 1 0 1 1]\n",
      "76 + 79 = 0\n",
      "------------\n",
      "iters:5200\n",
      "Loss:0.5\n",
      "A [0 0 0 1 0 0 0 0]  B [0 1 1 1 0 0 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 0 0 0 0 0]\n",
      "16 + 112 = 0\n",
      "------------\n",
      "iters:5300\n",
      "Loss:2.0\n",
      "A [0 1 1 1 0 0 0 0]  B [0 0 1 0 1 0 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 1 1 0 0 1]\n",
      "112 + 41 = 0\n",
      "------------\n",
      "iters:5400\n",
      "Loss:2.0\n",
      "A [0 1 0 0 0 1 1 0]  B [0 1 0 0 0 0 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 0 0 1 1 1]\n",
      "70 + 65 = 0\n",
      "------------\n",
      "iters:5500\n",
      "Loss:1.5\n",
      "A [0 1 0 1 0 1 1 1]  B [0 0 1 0 1 1 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 0 0 0 1 1]\n",
      "87 + 44 = 0\n",
      "------------\n",
      "iters:5600\n",
      "Loss:1.0\n",
      "A [0 0 0 0 0 0 1 1]  B [0 0 0 0 0 1 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 0 0 0 1 0 1 0]\n",
      "3 + 7 = 0\n",
      "------------\n",
      "iters:5700\n",
      "Loss:1.5\n",
      "A [0 0 0 1 0 1 1 0]  B [0 0 0 1 0 1 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 0 1 0 1 0 1 0]\n",
      "22 + 20 = 0\n",
      "------------\n",
      "iters:5800\n",
      "Loss:1.5\n",
      "A [0 0 0 1 1 1 0 1]  B [0 0 0 1 0 1 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 0 1 1 0 0 1 0]\n",
      "29 + 21 = 0\n",
      "------------\n",
      "iters:5900\n",
      "Loss:1.0\n",
      "A [0 0 0 0 1 1 0 0]  B [0 0 0 1 0 1 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 0 1 0 0 0 1 0]\n",
      "12 + 22 = 0\n",
      "------------\n",
      "iters:6000\n",
      "Loss:2.5\n",
      "A [0 1 0 1 1 0 0 0]  B [0 1 1 0 1 1 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 1 0 0 0 1 1 1]\n",
      "88 + 111 = 0\n",
      "------------\n",
      "iters:6100\n",
      "Loss:1.0\n",
      "A [0 0 1 1 0 0 1 1]  B [0 0 0 0 1 1 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 0 0 0 0 0 1]\n",
      "51 + 14 = 0\n",
      "------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iters:6200\n",
      "Loss:3.0\n",
      "A [0 1 0 1 0 1 1 0]  B [0 1 1 0 1 0 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 1 1 1 1 0]\n",
      "86 + 104 = 0\n",
      "------------\n",
      "iters:6300\n",
      "Loss:1.5\n",
      "A [0 1 1 1 1 1 0 0]  B [0 0 1 0 0 1 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 0 0 0 1 0]\n",
      "124 + 38 = 0\n",
      "------------\n",
      "iters:6400\n",
      "Loss:1.5\n",
      "A [0 1 0 1 0 0 1 0]  B [0 0 0 1 0 0 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 1 0 0 0 1 0]\n",
      "82 + 16 = 0\n",
      "------------\n",
      "iters:6500\n",
      "Loss:1.0\n",
      "A [0 0 0 1 1 1 0 0]  B [0 1 1 0 0 1 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 0 0 0 1 0]\n",
      "28 + 102 = 0\n",
      "------------\n",
      "iters:6600\n",
      "Loss:2.5\n",
      "A [0 1 1 0 0 0 1 1]  B [0 0 0 1 0 1 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 1 1 1 0 0 1]\n",
      "99 + 22 = 0\n",
      "------------\n",
      "iters:6700\n",
      "Loss:2.0\n",
      "A [0 1 0 1 1 0 1 0]  B [0 1 1 1 1 0 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 1 0 1 0 1 0 0]\n",
      "90 + 122 = 0\n",
      "------------\n",
      "iters:6800\n",
      "Loss:2.0\n",
      "A [0 1 1 0 0 0 1 0]  B [0 1 0 0 1 0 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 0 1 0 1 0]\n",
      "98 + 72 = 0\n",
      "------------\n",
      "iters:6900\n",
      "Loss:1.0\n",
      "A [0 0 1 0 1 1 0 0]  B [0 1 1 0 0 1 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 1 0 0 0 0]\n",
      "44 + 100 = 0\n",
      "------------\n",
      "iters:7000\n",
      "Loss:3.5\n",
      "A [0 0 0 0 1 1 0 1]  B [0 1 1 1 0 0 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 1 1 1 1 1 1]\n",
      "13 + 114 = 0\n",
      "------------\n",
      "iters:7100\n",
      "Loss:2.5\n",
      "A [0 1 0 0 1 0 1 1]  B [0 0 1 0 1 0 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 1 1 0 1 0 1]\n",
      "75 + 42 = 0\n",
      "------------\n",
      "iters:7200\n",
      "Loss:3.0\n",
      "A [0 1 1 1 0 0 1 1]  B [0 1 0 0 1 0 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 1 1 1 0 1]\n",
      "115 + 74 = 0\n",
      "------------\n",
      "iters:7300\n",
      "Loss:2.5\n",
      "A [0 1 1 1 1 1 1 1]  B [0 1 0 1 0 1 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 1 0 1 0 1 0 1]\n",
      "127 + 86 = 0\n",
      "------------\n",
      "iters:7400\n",
      "Loss:1.0\n",
      "A [0 0 1 0 0 1 1 0]  B [0 0 1 1 1 0 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 1 0 0 0 0 0]\n",
      "38 + 58 = 0\n",
      "------------\n",
      "iters:7500\n",
      "Loss:2.0\n",
      "A [0 1 0 1 0 0 1 1]  B [0 1 1 0 0 0 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 1 0 1 0 0]\n",
      "83 + 97 = 0\n",
      "------------\n",
      "iters:7600\n",
      "Loss:2.0\n",
      "A [0 1 1 1 0 1 1 0]  B [0 1 1 1 0 0 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 1 1 0 1 0 0 0]\n",
      "118 + 114 = 0\n",
      "------------\n",
      "iters:7700\n",
      "Loss:2.5\n",
      "A [0 1 1 1 0 1 0 0]  B [0 1 0 1 0 1 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 1 0 0 1 0 1 1]\n",
      "116 + 87 = 0\n",
      "------------\n",
      "iters:7800\n",
      "Loss:2.5\n",
      "A [0 1 0 1 0 0 0 0]  B [0 1 1 0 0 0 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 1 0 0 1 1]\n",
      "80 + 99 = 0\n",
      "------------\n",
      "iters:7900\n",
      "Loss:2.0\n",
      "A [0 0 0 1 0 0 1 1]  B [0 1 0 0 0 0 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 0 1 0 1 1 0]\n",
      "19 + 67 = 0\n",
      "------------\n",
      "iters:8000\n",
      "Loss:2.5\n",
      "A [0 1 0 0 1 0 0 0]  B [0 1 1 0 1 1 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 1 0 1 1 0]\n",
      "72 + 110 = 0\n",
      "------------\n",
      "iters:8100\n",
      "Loss:1.5\n",
      "A [0 0 0 0 0 1 1 0]  B [0 1 0 0 1 1 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 0 1 0 1 0 0]\n",
      "6 + 78 = 0\n",
      "------------\n",
      "iters:8200\n",
      "Loss:1.5\n",
      "A [0 1 0 0 1 1 0 0]  B [0 1 0 1 1 1 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 0 1 0 0 0]\n",
      "76 + 92 = 0\n",
      "------------\n",
      "iters:8300\n",
      "Loss:2.0\n",
      "A [0 1 1 1 1 1 0 0]  B [0 0 1 1 0 0 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 0 1 1 0 0]\n",
      "124 + 48 = 0\n",
      "------------\n",
      "iters:8400\n",
      "Loss:2.0\n",
      "A [0 0 1 0 1 1 1 0]  B [0 0 1 0 1 1 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 0 1 1 0 1 0]\n",
      "46 + 44 = 0\n",
      "------------\n",
      "iters:8500\n",
      "Loss:3.0\n",
      "A [0 1 1 1 0 1 0 1]  B [0 1 0 0 1 0 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 1 1 1 1 0]\n",
      "117 + 73 = 0\n",
      "------------\n",
      "iters:8600\n",
      "Loss:0.5\n",
      "A [0 1 0 0 1 0 0 0]  B [0 0 1 1 1 0 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 0 0 0 0 0 0]\n",
      "72 + 56 = 0\n",
      "------------\n",
      "iters:8700\n",
      "Loss:2.0\n",
      "A [0 1 0 1 0 0 0 1]  B [0 1 1 1 0 0 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 1 0 0 0 0 1 1]\n",
      "81 + 114 = 0\n",
      "------------\n",
      "iters:8800\n",
      "Loss:2.0\n",
      "A [0 0 0 0 1 0 1 0]  B [0 1 0 1 0 0 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 0 1 1 0 1 0]\n",
      "10 + 80 = 0\n",
      "------------\n",
      "iters:8900\n",
      "Loss:3.0\n",
      "A [0 1 1 1 1 1 0 1]  B [0 1 1 1 1 0 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 1 1 1 0 1 0 1]\n",
      "125 + 120 = 0\n",
      "------------\n",
      "iters:9000\n",
      "Loss:2.5\n",
      "A [0 1 0 1 0 1 1 0]  B [0 0 0 1 0 1 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 1 0 1 0 1 1]\n",
      "86 + 21 = 0\n",
      "------------\n",
      "iters:9100\n",
      "Loss:3.0\n",
      "A [0 1 1 1 0 1 0 0]  B [0 1 1 1 1 0 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 1 1 0 1 1 0 1]\n",
      "116 + 121 = 0\n",
      "------------\n",
      "iters:9200\n",
      "Loss:2.5\n",
      "A [0 0 0 1 0 1 1 1]  B [0 0 1 0 0 1 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 0 1 1 1 1 1 0]\n",
      "23 + 39 = 0\n",
      "------------\n",
      "iters:9300\n",
      "Loss:3.0\n",
      "A [0 1 1 0 1 0 0 0]  B [0 0 0 0 1 1 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 1 1 0 1 1 1]\n",
      "104 + 15 = 0\n",
      "------------\n",
      "iters:9400\n",
      "Loss:2.0\n",
      "A [0 0 0 0 1 0 0 1]  B [0 0 1 0 1 1 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 0 1 1 0 1 0 1]\n",
      "9 + 44 = 0\n",
      "------------\n",
      "iters:9500\n",
      "Loss:2.5\n",
      "A [0 1 1 0 0 1 0 1]  B [0 1 1 0 1 1 1 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 1 0 1 0 0 1 1]\n",
      "101 + 110 = 0\n",
      "------------\n",
      "iters:9600\n",
      "Loss:3.0\n",
      "A [0 0 1 0 0 1 1 0]  B [0 1 0 1 0 0 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[0 1 1 1 0 1 1 1]\n",
      "38 + 81 = 0\n",
      "------------\n",
      "iters:9700\n",
      "Loss:2.0\n",
      "A [0 1 1 1 0 1 0 1]  B [0 0 1 1 0 0 0 0]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 0 0 1 0 1]\n",
      "117 + 48 = 0\n",
      "------------\n",
      "iters:9800\n",
      "Loss:1.5\n",
      "A [0 0 1 0 1 1 0 1]  B [0 1 1 1 1 0 1 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 0 1 0 0 0]\n",
      "45 + 123 = 0\n",
      "------------\n",
      "iters:9900\n",
      "Loss:2.5\n",
      "A [0 1 1 1 0 0 0 1]  B [0 1 0 0 0 1 0 1]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 1 0 1 1 0]\n",
      "113 + 69 = 0\n",
      "------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x287ecd57688>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from common import functions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# def d_tanh(x):\n",
    "\n",
    "\n",
    "\n",
    "# データを用意\n",
    "# 2進数の桁数\n",
    "binary_dim = 8\n",
    "# 最大値 + 1\n",
    "largest_number = pow(2, binary_dim)\n",
    "# largest_numberまで2進数を用意\n",
    "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "\n",
    "input_layer_size = 2\n",
    "hidden_layer_size = 16\n",
    "output_layer_size = 1\n",
    "\n",
    "weight_init_std = 1\n",
    "learning_rate = 0.1\n",
    "\n",
    "iters_num = 10000\n",
    "plot_interval = 100\n",
    "\n",
    "# ウェイト初期化 (バイアスは簡単のため省略)\n",
    "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
    "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
    "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
    "\n",
    "# Xavier\n",
    "\n",
    "\n",
    "# He\n",
    "\n",
    "\n",
    "\n",
    "# 勾配\n",
    "W_in_grad = np.zeros_like(W_in)\n",
    "W_out_grad = np.zeros_like(W_out)\n",
    "W_grad = np.zeros_like(W)\n",
    "\n",
    "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
    "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
    "y = np.zeros((output_layer_size, binary_dim))\n",
    "\n",
    "delta_out = np.zeros((output_layer_size, binary_dim))\n",
    "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
    "\n",
    "all_losses = []\n",
    "\n",
    "for i in range(iters_num):\n",
    "    \n",
    "    # A, B初期化 (a + b = d)\n",
    "    a_int = np.random.randint(largest_number/2)\n",
    "    a_bin = binary[a_int] # binary encoding\n",
    "    b_int = np.random.randint(largest_number/2)\n",
    "    b_bin = binary[b_int] # binary encoding\n",
    "    \n",
    "    # 正解データ\n",
    "    d_int = a_int + b_int\n",
    "    d_bin = binary[d_int]\n",
    "    \n",
    "    # 出力バイナリ\n",
    "    out_bin = np.zeros_like(d_bin)\n",
    "    \n",
    "    # 時系列全体の誤差\n",
    "    all_loss = 0    \n",
    "    \n",
    "    # 時系列ループ\n",
    "    for t in range(binary_dim):\n",
    "        # 入力値\n",
    "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
    "        # 時刻tにおける正解データ\n",
    "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
    "        \n",
    "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
    "        z[:,t+1] = functions.relu(u[:,t+1])\n",
    "\n",
    "        y[:,t] = functions.relu(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
    "\n",
    "\n",
    "        #誤差\n",
    "        loss = functions.mean_squared_error(dd, y[:,t])\n",
    "        \n",
    "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_relu(y[:,t])        \n",
    "        \n",
    "        all_loss += loss\n",
    "\n",
    "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
    "    \n",
    "    \n",
    "    for t in range(binary_dim)[::-1]:\n",
    "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
    "\n",
    "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + \n",
    "                      np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
    "\n",
    "        # 勾配更新\n",
    "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
    "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
    "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
    "    \n",
    "    # 勾配適用\n",
    "    W_in -= learning_rate * W_in_grad\n",
    "    W_out -= learning_rate * W_out_grad\n",
    "    W -= learning_rate * W_grad\n",
    "    \n",
    "    W_in_grad *= 0\n",
    "    W_out_grad *= 0\n",
    "    W_grad *= 0\n",
    "    \n",
    "\n",
    "    if(i % plot_interval == 0):\n",
    "        all_losses.append(all_loss)        \n",
    "        print(\"iters:\" + str(i))\n",
    "        print(\"Loss:\" + str(all_loss))\n",
    "        print(\"A\", a_bin, \" B\", b_bin)\n",
    "        print(\"Pred:\" + str(out_bin))\n",
    "        print(\"True:\" + str(d_bin))\n",
    "        out_int = 0\n",
    "        for index,x in enumerate(reversed(out_bin)):\n",
    "            out_int += x * pow(2, index)\n",
    "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
    "        print(\"------------\")\n",
    "\n",
    "lists = range(0, iters_num, plot_interval)\n",
    "plt.plot(lists, all_losses, label=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-retrieval",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
